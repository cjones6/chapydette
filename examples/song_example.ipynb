{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Chapydette Song Example </center>\n",
    "Chapydette is a fast and flexible changepoint detection package. In this notebook we'll take a look at how to use the changepoint detection methods in Chapydette to detect changepoints in the song Hakuna Matata. You can listen to the song here: https://www.youtube.com/watch?v=ITDD4doC6iw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To install Chapydette, simply run `setup.py install`, possibly with `sudo`. The installer will check whether you have the required dependencies. There are three optional dependencies:\n",
    "- Faiss https://github.com/facebookresearch/faiss\n",
    "- Yael http://yael.gforge.inria.fr/\n",
    "- Pomegranate https://pomegranate.readthedocs.io/en/latest/index.html\n",
    "\n",
    "If you have them installed and use them, they will greatly speed up the feature generation code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the raw features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Chapydette's examples folder includes a csv file data/hakuna_matata.pickle that contains pitches, loudness and timbre features generated by Echo Nest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import cPickle as pickle\n",
    "\n",
    "song_path = 'data/hakuna_matata.pickle'\n",
    "song = pickle.load(open(song_path, 'rb'))\n",
    "song_data = song['data']\n",
    "song_times = song['times']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row represents the values of the feature at one time point. The rows are ordered by time, with the first row representing the features at the start of the song. There are 840 observations and 26 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of song features: (840, 26)\n",
      "Sample of features: [[ 5.35000e-01  6.26000e-01  5.82000e-01  4.79000e-01  5.72000e-01\n",
      "   1.00000e+00  8.15000e-01  4.97000e-01  5.64000e-01  6.35000e-01\n",
      "   6.90000e-01  4.51000e-01  0.00000e+00  1.71130e+02  9.46900e+00\n",
      "  -2.84800e+01  5.74910e+01 -5.00670e+01  1.48330e+01  5.35900e+00\n",
      "  -2.72280e+01  9.73000e-01 -1.06400e+01 -7.22800e+00 -6.00000e+01\n",
      "  -6.00000e+01]\n",
      " [ 1.57000e-01  2.47000e-01  4.06000e-01  1.00000e+00  2.91000e-01\n",
      "   8.30000e-02  1.20000e-01  1.56000e-01  3.80000e-02  4.30000e-02\n",
      "   2.49000e-01  1.53000e-01  3.48260e+01  1.04290e+01 -1.18830e+01\n",
      "   3.11284e+02  7.02560e+01  8.53900e+00 -3.20400e+01  5.50860e+01\n",
      "  -7.36300e+00 -2.21440e+01 -5.70240e+01  1.02710e+01 -6.00000e+01\n",
      "  -9.51100e+00]\n",
      " [ 4.96000e-01  3.70000e-01  2.60000e-01  2.89000e-01  2.60000e-01\n",
      "   3.42000e-01  8.16000e-01  1.00000e+00  5.92000e-01  6.54000e-01\n",
      "   5.71000e-01  2.45000e-01  3.46040e+01  1.24793e+02  7.14490e+01\n",
      "   1.42850e+01  3.82700e+01  7.28560e+01  3.34580e+01  2.52190e+01\n",
      "  -7.01490e+01  1.94300e+01  1.63500e+00  8.36500e+00 -5.28720e+01\n",
      "  -1.83960e+01]]\n"
     ]
    }
   ],
   "source": [
    "print('Shape of song features:', song_data.shape)\n",
    "print('Sample of features:', song_data[0:3, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature generation\n",
    "Prior to applying the changepoint detection algorithms, we want to generate more appropriate features based on the raw features above. Chapydette provides three options: bag-of-features, VLAD, and Fisher vectors. We'll try bag-of-features. Bag-of-features creates histograms for sliding windows. To do so, it does the following:\n",
    "1. Standardizes the data and runs PCA\n",
    "2. Runs k-means to create a \"codebook\"\n",
    "3. Finds the nearest centroid to each observation\n",
    "4. Forms a sliding window across the observations and creates a histogram within each sliding window based on the results of the previous step\n",
    "\n",
    "To use the code it is not necessary to understand the above steps. It is only necessary to understand the required inputs. The required inputs are:\n",
    "- data_features: The raw features\n",
    "- nclusters: The number of bins to use for the histograms\n",
    "- window_length: The length of the sliding window. This is terms of time if you input a vector for `times` (see below) or in terms of indices otherwise.\n",
    "\n",
    "The most noteworthy optional inputs with their defaults in parentheses are:\n",
    "- data_codebook (None): Dataset to use for codebook generation step. If not specified, the code uses the same data for the codebook generation step and the feature generation step. \n",
    "- times (None): Times corresponding to each data point. If you want the sliding windows to be based on time rather than index, you need to specify a vector of times, with one time for each observation. For example, for this song we could input the times in the song at which all of the observations were recorded.\n",
    "- do_pca (True): Whether to perform PCA on the data\n",
    "- window_overlap (0.2*window_length): Sliding window overlap, in terms of time.            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our music example, we'll use the following:\n",
    "- data_features = the raw features we loaded above\n",
    "- nclusters = 16. In practice we would want to use many songs for the codebook generation. We would set data_codebook to be the concatenation of the raw features from those songs.\n",
    "- window_length = 10s\n",
    "- data_codebook = None\n",
    "- times = the times in the song at which the features were recorded\n",
    "- do_pca = True\n",
    "- window_overlap = 0.2*window_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the feature generation step consists of several things:\n",
    "- features: The features that you should input into the changepoint detection algorithm\n",
    "- interval_start_times: Start time of every sliding window  \n",
    "- interval_end_times: End time of every sliding window\n",
    "- scaler: The Scikit-Learn scaler used to standardize the data\n",
    "- pca: The trained PCA function\n",
    "- centroids: The centroids from the codebook generation step\n",
    "\n",
    "The latter three outputs will not be relevant to us now. They could be useful if you are processing data in batches and need to be able to generate features in the same way later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to load GPU Faiss: No module named swigfaiss_gpu\n",
      "Faiss falling back to CPU-only.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Faiss\n",
      "Using pomegranate\n",
      "Not using Yael\n",
      "Running PCA...\n",
      "Running k-means...\n",
      "Generating histograms...\n",
      "Done generating features.\n"
     ]
    }
   ],
   "source": [
    "from chapydette import feature_generation\n",
    "window_length = 5\n",
    "nclusters = 16\n",
    "features, interval_start_times, interval_end_times, scaler, pca, centroids = feature_generation.bag_of_features(song_data, nclusters, \n",
    "                                                                                            window_length, times=song_times, seed=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changepoint detection\n",
    "Chapydette currently contains two different kernel-based changepoint algorithms. The first one is based on the Maximum Mean Discrpancy (MMD) and computes the location of one changepoint (Gretton et al. (2012), Harchaoui et al. (2009)). The second is an implementation of the multiple kernel changepoint estimation algorithm of Harchaoui and Capp√© (2007). In this second algorithm you currently must specify the number of changepoints you want to detect. Both algorithms may be called from the cp_estimation module. \n",
    "\n",
    "The potential inputs to both algorithms with their defaults in parentheses are:\n",
    "- X (None): Matrix of observations. Each observation is one row.\n",
    "- gram (None): Pre-computed gram matrix\n",
    "- n_cp (1): Number of changepoints to detect\n",
    "- kernel_type ('detect'): Type of kernel to use. One of: 'precomputed', 'chi-squared', 'gaussian-euclidean', 'gaussian-tv', 'linear'\n",
    "- bw (None): Bandwidth for the kernel (if applicable)\n",
    "- min_dist (1): Minimum allowable distance between successive changepoints\n",
    "\n",
    "You must provide either X or gram. If you do not specify kernel_type, the code will try to determine an appropriate kernel to use. If you do not provide a bandwidth, the code will try to use a rule of thumb for the kernel. \n",
    "\n",
    "The outputs of the algorithms are the estimated changepoints, in terms of the indices of the observations. The estimated changepoints are the indices of the last element in each estimated segment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first try the MMD-based algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the Gaussian kernel with the Hellinger distance.\n",
      "Bandwidth not specified. Using a rule of thumb.\n",
      "Estimated last element in the first segment is 44\n"
     ]
    }
   ],
   "source": [
    "from chapydette import cp_estimation\n",
    "cp = cp_estimation.mmd_cpd(X=features, gram=None, n_cp=1, kernel_type='detect', bw=None, min_dist=1)\n",
    "print('Estimated last element in the first segment is', cp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm detected that there is a changepoint between the cpth and (cp+1)th observations (time intervals). Let's now use the `convert_cps_to_time` function in utils to see what time in the song this changepoint corresponds to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated changepoint time: 178.13747999999998 seconds\n"
     ]
    }
   ],
   "source": [
    "from chapydette import utils\n",
    "cp_times = utils.convert_cps_to_time([cp], interval_start_times, interval_end_times)\n",
    "\n",
    "print('Estimated changepoint time:', cp_times[0], 'seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I encourage you to listen to the song to verify that there is a changepoint at approximately this time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's try the multiple kernel changepoint algorithm. This time we'll search for five changepoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the Gaussian kernel with the Hellinger distance.\n",
      "Bandwidth not specified. Using a rule of thumb.\n",
      "Estimated changepoint times: [42.2722, 102.28687, 138.05104, 174.25135999999998, 206.240545]\n"
     ]
    }
   ],
   "source": [
    "cps = cp_estimation.mkcpe(X=features, gram=None, n_cp=5, kernel_type='detect', bw=None, min_dist=1)\n",
    "cp_times = utils.convert_cps_to_time(cps, interval_start_times, interval_end_times)\n",
    "\n",
    "print('Estimated changepoint times:', cp_times)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other options\n",
    "There are a lot of other things we could've tried, including different feature generation methods and different kernels. Below are two other options. Feel free to try more yourself and see how the results compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running PCA...\n",
      "Running k-means...\n",
      "Generating VLAD features...\n",
      "Done generating features.\n",
      "[27.970750000000002, 59.805350000000004, 100.18517, 139.90345000000002, 180.06254]\n"
     ]
    }
   ],
   "source": [
    "window_length = 10\n",
    "nclusters = 8\n",
    "features, interval_start_times, interval_end_times, scaler, pca, centroids = feature_generation.vlad(song_data, nclusters, \n",
    "                                                                                            window_length, times=song_times, seed=0)\n",
    "cps = cp_estimation.mkcpe(X=features, gram=None, n_cp=5, kernel_type='linear', bw=None, min_dist=1)\n",
    "cp_times = utils.convert_cps_to_time(cps, interval_start_times, interval_end_times)\n",
    "print(cp_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running PCA...\n",
      "Running GMM...\n",
      "Generating Fisher vector features...\n",
      "Done generating features.\n",
      "[20.14356, 43.929024999999996, 100.18517, 139.90345000000002, 180.06254]\n"
     ]
    }
   ],
   "source": [
    "window_length = 10\n",
    "nclusters = 8\n",
    "features, interval_start_times, interval_end_times, scaler, pca, centroids = feature_generation.fisher_vectors(song_data, nclusters, \n",
    "                                                                                            window_length, times=song_times, seed=0)\n",
    "cps = cp_estimation.mkcpe(X=features, gram=None, n_cp=5, kernel_type='linear', bw=None, min_dist=1)\n",
    "cp_times = utils.convert_cps_to_time(cps, interval_start_times, interval_end_times)\n",
    "print(cp_times)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References:\n",
    "- Gretton, A., Borgwardt, K. M., Rasch, M. J., Sch√∂lkopf, B., & Smola, A. (2012). A kernel two-sample test. Journal of Machine Learning Research, 13(Mar), 723-773.\n",
    "- Harchaoui, Z., & Capp√©, O. (2007, August). Retrospective multiple change-point estimation with kernels. In IEEE/SP 14th Workshop on Statistical Signal Processing, 2007. SSP'07. (pp. 768-772). IEEE.\n",
    "- Harchaoui, Z., Moulines, E., & Bach, F. R. (2009). Kernel change-point analysis. In Advances in Neural Information Processing Systems (pp. 609-616)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
