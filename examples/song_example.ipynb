{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Chapydette Song Example </center>\n",
    "Chapydette is a fast and flexible change-point estimation package. In this notebook we'll take a look at how to use the change-point estimation methods in Chapydette to estimate the locations of change points in the song Hakuna Matata. You can listen to the song here: https://www.youtube.com/watch?v=ITDD4doC6iw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the raw features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Chapydette's examples folder includes a csv file data/hakuna_matata.pickle that contains pitches, loudness and timbre features generated by Echo Nest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "song_path = 'data/hakuna_matata.pickle'\n",
    "try:\n",
    "    song = pickle.load(open(song_path, 'rb'), encoding='latin1')\n",
    "except:\n",
    "    song = pickle.load(open(song_path, 'rb'))\n",
    "song_data = song['data']\n",
    "song_times = song['times']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row represents the values of the feature at one time point. The rows are ordered by time, with the first row representing the features at the start of the song. There are 840 observations and 26 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of song features: (840, 26)\n",
      "Sample of features: [[ 5.35000e-01  6.26000e-01  5.82000e-01  4.79000e-01  5.72000e-01\n",
      "   1.00000e+00  8.15000e-01  4.97000e-01  5.64000e-01  6.35000e-01\n",
      "   6.90000e-01  4.51000e-01  0.00000e+00  1.71130e+02  9.46900e+00\n",
      "  -2.84800e+01  5.74910e+01 -5.00670e+01  1.48330e+01  5.35900e+00\n",
      "  -2.72280e+01  9.73000e-01 -1.06400e+01 -7.22800e+00 -6.00000e+01\n",
      "  -6.00000e+01]\n",
      " [ 1.57000e-01  2.47000e-01  4.06000e-01  1.00000e+00  2.91000e-01\n",
      "   8.30000e-02  1.20000e-01  1.56000e-01  3.80000e-02  4.30000e-02\n",
      "   2.49000e-01  1.53000e-01  3.48260e+01  1.04290e+01 -1.18830e+01\n",
      "   3.11284e+02  7.02560e+01  8.53900e+00 -3.20400e+01  5.50860e+01\n",
      "  -7.36300e+00 -2.21440e+01 -5.70240e+01  1.02710e+01 -6.00000e+01\n",
      "  -9.51100e+00]\n",
      " [ 4.96000e-01  3.70000e-01  2.60000e-01  2.89000e-01  2.60000e-01\n",
      "   3.42000e-01  8.16000e-01  1.00000e+00  5.92000e-01  6.54000e-01\n",
      "   5.71000e-01  2.45000e-01  3.46040e+01  1.24793e+02  7.14490e+01\n",
      "   1.42850e+01  3.82700e+01  7.28560e+01  3.34580e+01  2.52190e+01\n",
      "  -7.01490e+01  1.94300e+01  1.63500e+00  8.36500e+00 -5.28720e+01\n",
      "  -1.83960e+01]]\n"
     ]
    }
   ],
   "source": [
    "print('Shape of song features:', song_data.shape)\n",
    "print('Sample of features:', song_data[0:3, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature generation\n",
    "Prior to applying the change-point estimation algorithms, we want to generate more appropriate features based on the raw features above. Chapydette provides three options: bag-of-features, VLAD, and Fisher vectors. We'll try bag-of-features. Bag-of-features creates histograms for sliding windows. To do so, it does the following:\n",
    "1. Standardizes the data and runs PCA\n",
    "2. Runs k-means to create a \"codebook\"\n",
    "3. Finds the nearest centroid to each observation\n",
    "4. Forms a sliding window across the observations and creates a histogram within each sliding window based on the results of the previous step\n",
    "\n",
    "To use the code it is not necessary to understand the above steps. It is only necessary to understand the required inputs. The required inputs are:\n",
    "- data_features: The raw features\n",
    "- nclusters: The number of bins to use for the histograms\n",
    "- window_length: The length of the sliding window. This is terms of time if you input a vector for `times` (see below) or in terms of indices otherwise.\n",
    "\n",
    "The most noteworthy optional inputs with their defaults in parentheses are:\n",
    "- data_codebook (None): Dataset to use for codebook generation step. If not specified, the code uses the same data for the codebook generation step and the feature generation step. \n",
    "- times (None): Times corresponding to each data point. If you want the sliding windows to be based on time rather than index, you need to specify a vector of times, with one time for each observation. For example, for this song we could input the times in the song at which all of the observations were recorded.\n",
    "- do_pca (True): Whether to perform PCA on the data\n",
    "- window_overlap (0.2*window_length): Sliding window overlap, in terms of time.            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our music example, we'll use the following:\n",
    "- data_features = the raw features we loaded above\n",
    "- nclusters = 16. In practice we would want to use many songs for the codebook generation. We would set data_codebook to be the concatenation of the raw features from those songs.\n",
    "- window_length = 10s\n",
    "- data_codebook = None\n",
    "- times = the times in the song at which the features were recorded\n",
    "- do_pca = True\n",
    "- window_overlap = 0.2*window_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the feature generation step consists of several things:\n",
    "- features: The features that you should input into the change-point estimation algorithm\n",
    "- interval_start_times: Start time of every sliding window  \n",
    "- interval_end_times: End time of every sliding window\n",
    "- scaler: The Scikit-Learn scaler used to standardize the data\n",
    "- pca: The trained PCA function\n",
    "- centroids: The centroids from the codebook generation step\n",
    "\n",
    "The latter three outputs will not be relevant to us now. They could be useful if you are processing data in batches and need to be able to generate features in the same way later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running PCA...\n",
      "Running k-means...\n",
      "Generating histograms...\n",
      "Done generating features.\n"
     ]
    }
   ],
   "source": [
    "from chapydette import feature_generation\n",
    "window_length = 5\n",
    "nclusters = 16\n",
    "features, interval_start_times, interval_end_times, scaler, pca, centroids = feature_generation.bag_of_features(song_data, nclusters, \n",
    "                                                                                            window_length, times=song_times, seed=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change-point estimation\n",
    "Chapydette currently contains two different kernel-based change-point algorithms. The first one is based on the Maximum Mean Discrpancy (MMD) and computes the location of one change point (Gretton et al. (2012), Harchaoui et al. (2009)). The second is an implementation of the multiple kernel change-point estimation algorithm of Harchaoui and Cappé (2007). Both algorithms may be called from the cp_estimation module.\n",
    "\n",
    "The potential inputs to both algorithms with their defaults in parentheses are:\n",
    "- X (None): Matrix of observations. Each observation is one row.\n",
    "- gram (None): Pre-computed gram matrix\n",
    "- n_cp (1): Number of change points to estimate\n",
    "- est_ncp (False): Whether to estimate the number of change points using the method of Arlot et al. (2019).\n",
    "- kernel_type ('detect'): Type of kernel to use. One of: 'precomputed', 'chi-squared', 'gaussian-euclidean', 'gaussian-tv', 'linear'\n",
    "- bw (None): Bandwidth for the kernel (if applicable)\n",
    "- min_dist (1): Minimum allowable distance between successive change points\n",
    "- alpha (2): Parameter in the method of Arlot et al. (2019) used to estimate the number of change points.\n",
    "- return_obj (False): Whether to return the optimal objective values.\n",
    "\n",
    "You must provide either X or gram. If you do not specify kernel_type, the code will try to determine an appropriate kernel to use. If you do not provide a bandwidth, the code will try to use a rule of thumb for the kernel. \n",
    "\n",
    "The outputs of the algorithms are the estimated change points, in terms of the indices of the observations. The estimated change points are the indices of the last element in each estimated segment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first try the MMD-based algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the Gaussian kernel with the Hellinger distance.\n",
      "Bandwidth not specified. Using a rule of thumb.\n",
      "Estimated last element in the first segment is 43\n"
     ]
    }
   ],
   "source": [
    "from chapydette import cp_estimation\n",
    "cp = cp_estimation.mmd_cpd(X=features, gram=None, n_cp=1, kernel_type='detect', bw=None, min_dist=1)\n",
    "print('Estimated last element in the first segment is', cp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm estimated that there is a change point between the cpth and (cp+1)th observations (time intervals). Let's now use the `convert_cps_to_time` function in utils to see what time in the song this change point corresponds to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated change-point time: 174.25135999999998 seconds\n"
     ]
    }
   ],
   "source": [
    "from chapydette import utils\n",
    "cp_times = utils.convert_cps_to_time([cp], interval_start_times, interval_end_times)\n",
    "\n",
    "print('Estimated change-point time:', cp_times[0], 'seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I encourage you to listen to the song to verify that there is a change point at approximately this time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's try the multiple kernel change-point algorithm. This time we'll search for five change points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the Gaussian kernel with the Hellinger distance.\n",
      "Bandwidth not specified. Using a rule of thumb.\n",
      "Estimated change-point times: [42.2722, 102.28687, 138.05104, 174.25135999999998, 202.160275]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "cps = cp_estimation.mkcpe(X=features, gram=None, n_cp=5, kernel_type='detect', bw=None, min_dist=1)\n",
    "cp_times = utils.convert_cps_to_time(cps.flatten(), interval_start_times, interval_end_times)\n",
    "\n",
    "print('Estimated change-point times:', cp_times)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other options\n",
    "There are a lot of other things we could've tried, including different feature generation methods and different kernels. Below are two other options. Feel free to try more yourself and see how the results compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running PCA...\n",
      "Running k-means...\n",
      "Generating VLAD features...\n",
      "Done generating features.\n",
      "[43.929024999999996, 84.08796, 100.18517, 139.90345000000002, 188.08968]\n"
     ]
    }
   ],
   "source": [
    "window_length = 10\n",
    "nclusters = 8\n",
    "features, interval_start_times, interval_end_times, scaler, pca, centroids = feature_generation.vlad(song_data, nclusters, \n",
    "                                                                                            window_length, times=song_times, seed=0)\n",
    "cps = cp_estimation.mkcpe(X=features, gram=None, n_cp=5, kernel_type='linear', bw=None, min_dist=1)\n",
    "cp_times = utils.convert_cps_to_time(cps.flatten(), interval_start_times, interval_end_times)\n",
    "print(cp_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running PCA...\n",
      "Running GMM...\n",
      "Generating Fisher vector features...\n",
      "Done generating features.\n",
      "[20.14356, 43.929024999999996, 100.18517, 139.90345000000002, 180.06254]\n"
     ]
    }
   ],
   "source": [
    "window_length = 10\n",
    "nclusters = 8\n",
    "features, interval_start_times, interval_end_times, scaler, pca, centroids = feature_generation.fisher_vectors(song_data, nclusters, \n",
    "                                                                                            window_length, times=song_times, seed=0)\n",
    "cps = cp_estimation.mkcpe(X=features, gram=None, n_cp=5, kernel_type='linear', bw=None, min_dist=1)\n",
    "cp_times = utils.convert_cps_to_time(cps.flatten(), interval_start_times, interval_end_times)\n",
    "print(cp_times)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References:\n",
    "- Gretton, A., Borgwardt, K. M., Rasch, M. J., Schölkopf, B., & Smola, A. (2012). A kernel two-sample test. Journal of Machine Learning Research, 13(Mar), 723-773.\n",
    "- Harchaoui, Z., & Cappé, O. (2007, August). Retrospective multiple change-point estimation with kernels. In IEEE/SP 14th Workshop on Statistical Signal Processing, 2007. SSP'07. (pp. 768-772). IEEE.\n",
    "- Harchaoui, Z., Moulines, E., & Bach, F. R. (2009). Kernel change-point analysis. In Advances in Neural Information Processing Systems (pp. 609-616)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
